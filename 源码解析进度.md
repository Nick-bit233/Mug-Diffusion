## （1）数据集准备部分
- 把下载好的数据集（osu谱面）放在data/文件夹下，可以准备一个子文件夹
- 制作beatmap.txt，记录谱面元数据，用于下一步
- 需要按照configs/mug/mania_beatmap_features.yaml中的配置，抽取谱面的特征
- 谱面特征需要转化为数据表，存放与beatmap.txt同一目录下的features.db数据库中
- ……（也许还有后续步骤）

### [mug/data]
mug/data/convertor.py: 工具类，用来解析单个.osu谱面文件
mug/data/dataset.py: 构建用于pytorch训练的数据集格式，在此之前，请确保已经抽取了谱面特征，并存放在features.db中
mug/data/utils.py: （待分析）一些用于对齐的工具函数

准备好的数据集条目示例：
```python
{
  "meta": {
    "path": "data/beatmap_4k/1469980 Silentroom vs. Frums - Aegleseeker/Silentroom vs. Frums - Aegleseeker ([Crz]Alleyne) [The Last Observation].osu", 
    "audio": "data/beatmap_4k/1469980 Silentroom vs. Frums - Aegleseeker/audio.ogg", 
    "game_mode": 3, "cs": 4.0, "version": "The Last Observation", "set_id": 1469980
  }, 
  "convertor": {
    "frame_ms": 46.439909297052154, "max_frame": 4096, "mirror": False, "random": False, "mirror_at_interval_prob": 0, "offset_ms": 0, "rate": 1.0
  }, 
  "note": array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 
  'valid_flag': array([1., 1., 1., ..., 0., 0., 0.]), 
  'audio': array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 
  'feature': array([ 24,  39,  41,  53,  55,  58,  81, 110, 124, 142, 157, 179, 192,
       212, 225, 237, 259, 275, 293, 314, 327])
}
```
处理音频的缓存`.npz`文件放置于`data/audio_cache`文件夹中。

### [scripts]

- 如果我的判断没错，准备数据这个阶段时完全手动的，看到这个文件夹下的脚本都没有被引用过……

scripts/prepare_beatmap.py: 用本地下载的osu谱面于制作beatmap.txt，给定输入输出文件夹
scripts/prepare_ranked_beatmap.py: 从osu网站上爬取谱面，制作beatmap.txt，需要用到kuit自己的网站服务
scripts/prepare_beatmap_from_ranking_mapper.py: 同上，不过似乎是从特定的谱师id中获取

scripts/prepare_beatmap_features.py: 从beatmap.txt中读取谱面元数据，抽取谱面特征，并提交到features.db中
抽取特征需要以下参数：
```
beat_map_path = beatmap.txt 文件的路径，记得铺面文件的存放位置与该txt位于同一目录
features_yaml = 配置feature文件的路径，如"configs/mug/mania_beatmap_features.yaml"
osu_tools = 一个osu官方开源的计算osu谱面信息的工具，在这里下载：https://github.com/ppy/osu-tools/tree/master/PerformanceCalculator
获得源码后，需要使用dotnet编译（参见仓库readme），
编译完成后，在\PerformanceCalculator\bin\Debug\net6.0\下可以找到PerformanceCalculator.dll文件，使用该文件的路径作为参数
ranked_map_path = 存储每个谱面的rank（谱面审核）信息，该信息需要通过预处理获取，如果没有，可以指定为None
dotnet_path = 为了运行PerformanceCalculator.dll，需要安装.NET6.0，安装并设置完成环境变量后，可直接使用填入"dotnet"
```

scripts/filter_beatmap.py: （待分析）似乎是用于过滤谱面中的变速
……
其他脚本似乎是用于处理Etterna谱面的，或者是一些用于算法测试的，跳过这一部分

### 数据集准备

如果你要训练，必须准备基本的两个数据集文件夹：
- `mug.data.dataset.OsuTrainDataset`
- `mug.data.dataset.OsuVaildDataset`

将数据集路径指定在训练用的配置文件中，目前的路径是`configs/mug/mug_diffusion.yaml`，
同时最好有一个`mug.data.dataset.OsuTestDataset`数据集，用于测试.

训练数据集示例结构：
> TODO

## （2）VAE编解码部分

这部分的目的是为了得到两个模型：
- Encoder：将谱面信息编码为一个向量
- Decoder：将一个向量解码为谱面信息

### [mug/firststage]
这部分用于实现AutoEncoder，自动编码器是一种无监督学习的神经网络模型，主要用于学习输入数据的压缩表示

AutoEncoder的结构如下：
![AutoEncoder](https://upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png)

其中包含两个`torch.nn.Module`类的组件：编码器（Encoder）和解码器（Decoder）。编码器的作用是将输入数据压缩为一个低维度的表示，而解码器则将这个低维度的表示恢复为原始的高维度数据

注意：mug/firststage/autoencoder.py中的部分模型使用的网络模块定义在mug/model/modules.py中

Encoder模型结构：

- 输入通道数16
- 经过一个3*3的卷积层，输出中间通道数64
- 经过一系列下采样操作：在每次下采样之前，先经过一个残差块，并让每两次下采样后，网络的通道数翻倍
  - 即: 64 * [1,1,2,2,4,4]
- 下采样网络：可选 卷积 或 平均池化
  - 卷积：3*3卷积层，stride=2
  - 平均池化：2*2平均池化层，stride=2
- 中间层：使用2个残差块，每个残差块包含两个3*3卷积层，通道数不变
- 输出层：首先通过一个归一化层对数据进行归一化处理，然后通过一个3*3卷积层将数据的通道数转换为编码器的输出通道数，输出 目标输出通道数*2 的向量表示
  - 如：目标输出通道数为32，则输出64维的向量表示

Decoder模型结构：

与Encoder模型结构正好相反，先将输入向量卷积到中间层，然后再经过一系列上采样操作，最后经过归一化和一个3*3卷积层，输出目标输出通道数的向量表示

损失函数：AutoEncoderKL类使用了两个损失函数，分别是重构损失和KL散度损失

- 重构损失：在`mug/firststage/losses.py`中定义，计算重构的谱面数据与原始谱面数据的误差，包括多种误差计算方式
- KL损失：计算编码器输出的均值和方差与标准正态分布的KL散度
  - KL损失在加入总损失时，需要乘以一个权重系数`self.kl_weight`，默认为0（不考虑KL损失）

AutoEncoder模型的使用：

> TODO


## （3）Diffusion训练部分
……

### [mug/cond]
这个文件夹里实现的都是条件编码模型，即将谱面的特征提示信息和音频信息一起编码，然后输入Diffusion模型的去噪过程，用来控制输出结果的条件向量。

> 特征输入：符合yaml格式的文本，里面的信息包括如 难度sr，long note比例，键形特征等
> 
> 音频输入：通过频谱变换离散化后的音频数据，有多种处理方式
> 
> noise level：
> 

- feature_cond.py: 
这段代码定义了一个名为`BeatmapFeatureEmbedder`的类，该类继承自`torch.nn.Module`，用于实现特征编码和嵌入。

这里嵌入的特征是谱面元数据，即`configs/mug/mania_beatmap_features.yaml`中定义的特征。该文件路径需要作为参数传入的构造函数中。    
读入`mania_beatmap_features.yaml`文件的信息后，会使用`mug/util.py`中的`count_beatmap_features`函数，计算每个特征需要在嵌入向量中用几个值来表示，例如，一个bool类型的特征需要3个值（表示True/False或者没有这个特征）。
使用` torch.nn.Embedding`创建一个嵌入层`self.embedding`，其维度为传入的参数的`embed_dim`。

```python
def __init__(self, path_to_yaml, embed_dim):
    super().__init__()
    with open(path_to_yaml) as f:
        self.feature_dicts = yaml.safe_load(f)
    self.embedding = torch.nn.Embedding(count_beatmap_features(self.feature_dicts), embed_dim)
```

`forward`方法是模型的前向传播过程。它接收一个输入`x`，将其转换为长整型，然后通过嵌入层进行转换。最后，使用`rearrange`函数调整张量的维度顺序。
pattern解释： 输入时二维向量，其形状为[B, F]，其中B代表批次大小，F代表特征数量。
经过嵌入层后，输出的形状为[B, F, H]，其中H代表嵌入向量的维度，即参数`embed_dim`。

```python
def forward(self, x):
    x = rearrange(self.embedding(x.long()), "b f h -> b h f") # [B, H, F]
    return x
```

`summary`方法使用`torchsummary`库的`summary`函数，打印出模型的概要信息，包括输出大小、参数数量、内核大小等。

```python
def summary(self):
    import torchsummary
    torchsummary.summary(self, input_data=[5, ],
                         dtypes=[torch.long],
                         col_names=("output_size", "num_params", "kernel_size"),
                         depth=10, device=torch.device("cpu"))
```

测试：输入一个特征配置文件，打印出模型的概要信息。

```python
if __name__ == '__main__':
    BeatmapFeatureEmbedder(path_to_yaml="configs/mug/mania_beatmap_features.yaml",
                           embed_dim=128).summary()
```
- wave.py: 定义`STFTEncoder`，`MelspectrogramEncoder`等模型，主要实现将音频信息编码为嵌入向量
  - STFTEncoder类是一个音频编码器，使用了短时傅里叶变换（STFT）进行音频编码。
  - MelspectrogramEncoder和MelspectrogramEncoder1D类是另外两种音频编码器，使用了梅尔频谱图（Melspectrogram）进行音频编码。
  - S4BidirectionalLayer类是一个双向S4模型层，它包含了一个归一化层和一个双向S4模型。
  - TimingDecoder类是一个解码器，它用于将编码后的音频数据解码回原始数据
  - MelspectrogramScaleEncoder1D类是一个一维的梅尔频谱图编码器，它使用了注意力机制进行音频编码。
实际编码音频应该使用了`MelspectrogramScaleEncoder1D`模型


## （4）WebUI和推理部分
……